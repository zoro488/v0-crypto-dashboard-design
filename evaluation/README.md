# ðŸ§ª CHRONOS AI Evaluation Framework

Framework de evaluaciÃ³n completo para el sistema CHRONOS - Sistema empresarial de gestiÃ³n financiera con IA.

![Tests](https://img.shields.io/badge/tests-6%2F6%20passing-brightgreen)
![Python](https://img.shields.io/badge/python-3.9%2B-blue)

## âš¡ Quick Start (Sin Dependencias de Azure)

```bash
# 1. Ir al directorio de evaluaciÃ³n
cd evaluation

# 2. Ejecutar pruebas unitarias (no requiere Azure)
python test_evaluators.py

# 3. Resultado esperado:
# Total: 6/6 tests passed (100%)
```

## ðŸš€ EvaluaciÃ³n Completa (Con Azure AI)

```bash
# 1. Instalar dependencias
pip install -r requirements.txt

# 2. Configurar Azure (opcional)
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"
export AZURE_OPENAI_API_KEY="your-key"

# 3. Ejecutar evaluaciÃ³n
python run_evaluation.py --service all
```

## ðŸ“‹ Estructura del Framework

```
evaluation/
â”œâ”€â”€ README.md                          # Esta documentaciÃ³n
â”œâ”€â”€ requirements.txt                   # Dependencias Python
â”œâ”€â”€ config.py                          # ConfiguraciÃ³n del framework
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ mega_ai_agent_queries.jsonl    # Queries de test para MegaAIAgent
â”‚   â”œâ”€â”€ form_automation_test.jsonl     # Tests para AIFormAutomation
â”‚   â”œâ”€â”€ business_logic_test.jsonl      # Tests de lÃ³gica de negocio
â”‚   â””â”€â”€ generate_datasets.py           # Script para generar datasets desde CSVs
â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ intent_detection.py            # Evaluador de detecciÃ³n de intenciones
â”‚   â”œâ”€â”€ business_logic.py              # Evaluador de lÃ³gica de negocio (distribuciÃ³n)
â”‚   â”œâ”€â”€ form_autofill.py               # Evaluador de auto-llenado de formularios
â”‚   â”œâ”€â”€ kpi_accuracy.py                # Evaluador de KPIs y dashboards
â”‚   â”œâ”€â”€ report_quality.py              # Evaluador de calidad de reportes
â”‚   â””â”€â”€ user_learning.py               # Evaluador de patrones de usuario
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ response_relevance.prompty     # Prompt para evaluar relevancia
â”‚   â””â”€â”€ report_quality.prompty         # Prompt para evaluar reportes
â”œâ”€â”€ run_evaluation.py                  # Script principal de evaluaciÃ³n
â””â”€â”€ results/                           # Directorio para resultados
```

## ðŸš€ Inicio RÃ¡pido

### 1. Instalar dependencias

```bash
cd evaluation
pip install -r requirements.txt
```

### 2. Configurar variables de entorno

```bash
# Para Azure OpenAI (recomendado)
export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"
export AZURE_OPENAI_API_KEY="your-api-key"
export AZURE_OPENAI_DEPLOYMENT="gpt-4"

# O para OpenAI
export OPENAI_API_KEY="your-api-key"
export OPENAI_MODEL="gpt-4"
```

### 3. Generar datasets de test

```bash
python datasets/generate_datasets.py
```

### 4. Ejecutar evaluaciÃ³n

```bash
python run_evaluation.py
```

## ðŸ“Š MÃ©tricas de EvaluaciÃ³n

### 1. Intent Detection Accuracy (MegaAIAgent)
EvalÃºa si el agente detecta correctamente las intenciones del usuario:
- `query_data`: Consultas de datos
- `create_record`: Crear registros
- `generate_report`: Generar reportes
- `navigate`: NavegaciÃ³n
- `analyze`: AnÃ¡lisis
- `help`: Ayuda
- `conversation`: ConversaciÃ³n general

### 2. Response Relevance
Mide si las respuestas son relevantes al contexto financiero de CHRONOS:
- Ventas y distribuciÃ³n
- Estado de bancos (7 bÃ³vedas)
- Inventario y stock
- Clientes y distribuidores

### 3. Business Logic Accuracy
Verifica las fÃ³rmulas de distribuciÃ³n de ventas:
```
montoBovedaMonte = precioCompraUnidad * cantidad
montoFletes = precioFlete * cantidad  
montoUtilidades = (precioVentaUnidad - precioCompraUnidad - precioFlete) * cantidad
```

### 4. Form Autofill Accuracy (AIFormAutomation)
EvalÃºa la precisiÃ³n del auto-llenado predictivo:
- Sugerencias basadas en patrones
- Validaciones de campos
- Campos requeridos vs opcionales

### 5. KPI Accuracy (AIPowerBI)
Verifica que los KPIs generados sean correctos:
- Total de ventas
- Capital por banco
- Stock bajo
- MÃ©tricas de clientes

### 6. Report Quality (AIScheduledReports)
EvalÃºa calidad de reportes:
- Completitud de datos
- Insights generados
- Formato y estructura

### 7. User Pattern Learning (UserLearning)
Mide detecciÃ³n de patrones:
- Acciones comunes
- Patrones de tiempo
- Patrones de navegaciÃ³n

## ðŸ”§ ConfiguraciÃ³n Avanzada

### Personalizar evaluadores

Edita `config.py` para ajustar:

```python
# Umbrales de evaluaciÃ³n
THRESHOLDS = {
    "intent_accuracy_min": 0.85,
    "relevance_min": 0.80,
    "business_logic_tolerance": 0.01,  # 1% de tolerancia
    "form_accuracy_min": 0.75,
}

# Servicios a evaluar
SERVICES_TO_EVALUATE = [
    "MegaAIAgent",
    "AIFormAutomation",
    "AIPowerBI",
    "AIScheduledReports",
    "UserLearning"
]
```

### Agregar nuevas mÃ©tricas

1. Crea un nuevo evaluador en `evaluators/`
2. Hereda de la clase base o implementa `__init__` y `__call__`
3. AgrÃ©galo en `run_evaluation.py`

## ðŸ“ˆ Interpretar Resultados

Los resultados se guardan en `results/evaluation_YYYYMMDD_HHMMSS.json`:

```json
{
  "summary": {
    "total_tests": 150,
    "passed": 142,
    "failed": 8,
    "pass_rate": 0.947
  },
  "metrics": {
    "intent_detection_accuracy": 0.92,
    "response_relevance": 0.88,
    "business_logic_accuracy": 0.99,
    "form_autofill_accuracy": 0.78,
    "kpi_accuracy": 0.95,
    "report_quality": 0.85,
    "user_learning_accuracy": 0.82
  },
  "details": {...}
}
```

## ðŸ”— IntegraciÃ³n con CI/CD

Agrega a tu pipeline:

```yaml
- name: Run AI Evaluation
  run: |
    cd evaluation
    pip install -r requirements.txt
    python run_evaluation.py --output results/
    python check_thresholds.py --min-pass-rate 0.90
```

## ðŸ“š Referencias

- [Azure AI Evaluation SDK](https://learn.microsoft.com/azure/ai-studio/how-to/evaluate-sdk)
- [CHRONOS System Documentation](../ESTRATEGIA_DEFINITIVA_V0_SPLINE_FIREBASE_COMPLETA.md)
- [Business Logic Formulas](../FORMULAS_CORRECTAS_VENTAS_Version2.md)
