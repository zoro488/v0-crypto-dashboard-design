#!/usr/bin/env python3
"""
Test Form Complete Evaluator
============================

Script de pruebas exhaustivas para el evaluador completo de formularios.
Verifica 100% de cobertura en:
- Todos los tipos de formularios (12 tipos)
- Validaciones Zod
- L√≥gica de negocio (distribuci√≥n de ventas)
- Estados de UI (wizard)
- C√°lculos autom√°ticos
- Sugerencias AI
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any
from dataclasses import dataclass

# Agregar path del proyecto
sys.path.insert(0, str(Path(__file__).parent.parent))

from evaluators.form_complete import FormCompleteEvaluator, FormType


@dataclass
class TestResult:
    """Resultado de un test."""
    test_id: str
    passed: bool
    expected_score: float
    actual_score: float
    metric: str
    details: str


def load_test_dataset(filepath: str) -> List[Dict[str, Any]]:
    """Carga dataset de pruebas JSONL."""
    tests = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                tests.append(json.loads(line))
    return tests


def run_form_tests() -> Dict[str, Any]:
    """Ejecuta todas las pruebas de formularios."""
    print("=" * 70)
    print("üß™ FORM COMPLETE EVALUATOR - TEST SUITE")
    print("=" * 70)
    
    # Inicializar evaluador
    evaluator = FormCompleteEvaluator(
        strict_mode=True,
        validate_business_rules=True,
        validate_ui_states=True
    )
    
    # Cargar dataset
    dataset_path = Path(__file__).parent / "datasets" / "form_complete_tests.jsonl"
    
    if not dataset_path.exists():
        print(f"‚ùå Dataset no encontrado: {dataset_path}")
        return {"error": "Dataset not found"}
    
    tests = load_test_dataset(str(dataset_path))
    print(f"\nüìä Cargadas {len(tests)} pruebas\n")
    
    # Resultados
    results = {
        "total": len(tests),
        "passed": 0,
        "failed": 0,
        "by_form_type": {},
        "by_metric": {},
        "failures": [],
        "details": []
    }
    
    # Categor√≠as de tests
    categories = {
        "venta": {"passed": 0, "total": 0},
        "cliente": {"passed": 0, "total": 0},
        "gasto": {"passed": 0, "total": 0},
        "transferencia": {"passed": 0, "total": 0},
        "producto": {"passed": 0, "total": 0},
        "orden_compra": {"passed": 0, "total": 0},
        "wizard": {"passed": 0, "total": 0},
        "suggestions": {"passed": 0, "total": 0},
        "calculations": {"passed": 0, "total": 0},
        "validations": {"passed": 0, "total": 0},
        "edge_cases": {"passed": 0, "total": 0},
    }
    
    # Ejecutar tests
    for i, test in enumerate(tests, 1):
        test_id = test.get("test_id", f"TEST-{i}")
        form_type = test.get("form_type")
        description = test.get("description", "Sin descripci√≥n")
        form_data = test.get("form_data", {})
        ground_truth = test.get("ground_truth", form_data)
        wizard_state = test.get("wizard_state")
        suggestions = test.get("suggestions")
        expected = test.get("expected", {})
        
        # Categorizar test
        category = form_type
        if "WIZARD" in test_id:
            category = "wizard"
        elif "SUGGEST" in test_id:
            category = "suggestions"
        elif "CALC" in test_id:
            category = "calculations"
        elif "VALID" in test_id:
            category = "validations"
        elif "EDGE" in test_id:
            category = "edge_cases"
        
        if category not in categories:
            categories[category] = {"passed": 0, "total": 0}
        categories[category]["total"] += 1
        
        # Ejecutar evaluaci√≥n
        try:
            result = evaluator(
                form_type=form_type,
                form_data=form_data,
                ground_truth=ground_truth,
                wizard_state=wizard_state,
                suggestions=suggestions
            )
            
            # Verificar m√©tricas esperadas
            test_passed = True
            fail_reason = []
            
            for metric, expected_value in expected.items():
                if metric == "overall_score":
                    actual_value = result.get("overall_score", 0)
                else:
                    actual_value = result.get("metrics", {}).get(metric, 0)
                
                # Tolerancia del 10% para scores parciales esperados
                if expected_value == 1.0:
                    if actual_value < 0.95:  # 95% para considerar "perfecto"
                        test_passed = False
                        fail_reason.append(f"{metric}: esperado ‚â•0.95, obtenido {actual_value:.2f}")
                elif expected_value == 0.0:
                    if actual_value > 0.1:  # Deber√≠a fallar
                        test_passed = False
                        fail_reason.append(f"{metric}: esperado ‚â§0.10, obtenido {actual_value:.2f}")
                else:
                    # Tolerancia del 15%
                    if abs(actual_value - expected_value) > 0.15:
                        test_passed = False
                        fail_reason.append(f"{metric}: esperado ~{expected_value:.2f}, obtenido {actual_value:.2f}")
            
            # Registrar resultado
            status = "‚úÖ" if test_passed else "‚ùå"
            
            if test_passed:
                results["passed"] += 1
                categories[category]["passed"] += 1
            else:
                results["failed"] += 1
                results["failures"].append({
                    "test_id": test_id,
                    "description": description,
                    "reasons": fail_reason
                })
            
            # Mostrar progreso
            overall = result.get("overall_score", 0)
            print(f"  {status} [{test_id}] {description[:40]}... Score: {overall:.2%}")
            
            results["details"].append({
                "test_id": test_id,
                "passed": test_passed,
                "overall_score": overall,
                "metrics": result.get("metrics", {})
            })
            
        except Exception as e:
            results["failed"] += 1
            print(f"  ‚ùå [{test_id}] ERROR: {str(e)}")
            results["failures"].append({
                "test_id": test_id,
                "description": description,
                "reasons": [f"Exception: {str(e)}"]
            })
    
    # Resumen
    print("\n" + "=" * 70)
    print("üìã RESUMEN DE RESULTADOS")
    print("=" * 70)
    
    pass_rate = (results["passed"] / results["total"]) * 100 if results["total"] > 0 else 0
    
    print(f"\n  Total Tests: {results['total']}")
    print(f"  ‚úÖ Passed: {results['passed']}")
    print(f"  ‚ùå Failed: {results['failed']}")
    print(f"  üìä Pass Rate: {pass_rate:.1f}%")
    
    # Por categor√≠a
    print("\n  üìÇ Por Categor√≠a:")
    for category, stats in sorted(categories.items()):
        if stats["total"] > 0:
            cat_rate = (stats["passed"] / stats["total"]) * 100
            status = "‚úÖ" if cat_rate >= 80 else "‚ö†Ô∏è" if cat_rate >= 50 else "‚ùå"
            print(f"    {status} {category}: {stats['passed']}/{stats['total']} ({cat_rate:.0f}%)")
    
    # Fallos detallados
    if results["failures"]:
        print("\n  ‚ö†Ô∏è Tests Fallidos:")
        for failure in results["failures"][:5]:  # Solo mostrar primeros 5
            print(f"    ‚Ä¢ [{failure['test_id']}] {failure['description'][:30]}")
            for reason in failure["reasons"]:
                print(f"      ‚Üí {reason}")
    
    # Status final
    print("\n" + "=" * 70)
    if pass_rate >= 95:
        print("üéâ RESULTADO: EXCELENTE (‚â•95%)")
    elif pass_rate >= 80:
        print("‚úÖ RESULTADO: APROBADO (‚â•80%)")
    elif pass_rate >= 60:
        print("‚ö†Ô∏è RESULTADO: ACEPTABLE (‚â•60%)")
    else:
        print("‚ùå RESULTADO: NECESITA MEJORAS (<60%)")
    print("=" * 70)
    
    results["pass_rate"] = pass_rate
    results["by_category"] = categories
    
    return results


def run_unit_tests():
    """Ejecuta tests unitarios espec√≠ficos."""
    print("\n" + "=" * 70)
    print("üî¨ UNIT TESTS - Funciones Espec√≠ficas")
    print("=" * 70)
    
    evaluator = FormCompleteEvaluator()
    passed = 0
    total = 0
    
    # Test 1: Distribuci√≥n de ventas (f√≥rmula correcta)
    print("\n1Ô∏è‚É£ Test Distribuci√≥n de Ventas (3 Bancos)")
    total += 1
    
    venta_data = {
        "cantidad": 10,
        "precioUnitario": 10000,
        "precioCompra": 6300,
        "precioFlete": 500,
        "distribucionBovedaMonte": 63000,  # 6300 √ó 10
        "distribucionFletes": 5000,         # 500 √ó 10
        "distribucionUtilidades": 32000,    # (10000-6300-500) √ó 10
        "precioTotal": 100000
    }
    
    result = evaluator(
        form_type="venta",
        form_data=venta_data,
        ground_truth=venta_data
    )
    
    business_accuracy = result["metrics"]["business_logic_accuracy"]
    if business_accuracy >= 0.95:
        print(f"   ‚úÖ Distribuci√≥n correcta: {business_accuracy:.2%}")
        passed += 1
    else:
        print(f"   ‚ùå Distribuci√≥n incorrecta: {business_accuracy:.2%}")
        for rule in result["details"]["business_rule_results"]:
            status = "‚úì" if rule["passed"] else "‚úó"
            print(f"      {status} {rule['rule']}: {rule['details']}")
    
    # Test 2: Validaci√≥n de email
    print("\n2Ô∏è‚É£ Test Validaci√≥n Email")
    total += 1
    
    # Email v√°lido
    cliente_valid = {"nombre": "Test User", "email": "test@example.com"}
    result_valid = evaluator(form_type="cliente", form_data=cliente_valid)
    
    # Email inv√°lido
    cliente_invalid = {"nombre": "Test User", "email": "invalid-email"}
    result_invalid = evaluator(form_type="cliente", form_data=cliente_invalid)
    
    if result_valid["metrics"]["validation_accuracy"] >= 0.9 and result_invalid["metrics"]["validation_accuracy"] < 1.0:
        print(f"   ‚úÖ Validaci√≥n email correcta")
        print(f"      Email v√°lido: {result_valid['metrics']['validation_accuracy']:.2%}")
        print(f"      Email inv√°lido: {result_invalid['metrics']['validation_accuracy']:.2%}")
        passed += 1
    else:
        print(f"   ‚ùå Validaci√≥n email incorrecta")
    
    # Test 3: Transferencia origen != destino
    print("\n3Ô∏è‚É£ Test Transferencia Bancos Diferentes")
    total += 1
    
    transfer_valid = {"bancoOrigen": "boveda_monte", "bancoDestino": "utilidades", "monto": 1000}
    transfer_invalid = {"bancoOrigen": "profit", "bancoDestino": "profit", "monto": 1000}
    
    result_valid = evaluator(form_type="transferencia", form_data=transfer_valid)
    result_invalid = evaluator(form_type="transferencia", form_data=transfer_invalid)
    
    if result_valid["metrics"]["business_logic_accuracy"] >= 0.9 and result_invalid["metrics"]["business_logic_accuracy"] < 1.0:
        print(f"   ‚úÖ Validaci√≥n transferencia correcta")
        print(f"      Bancos diferentes: {result_valid['metrics']['business_logic_accuracy']:.2%}")
        print(f"      Mismo banco: {result_invalid['metrics']['business_logic_accuracy']:.2%}")
        passed += 1
    else:
        print(f"   ‚ùå Validaci√≥n transferencia incorrecta")
    
    # Test 4: C√°lculo de margen
    print("\n4Ô∏è‚É£ Test C√°lculo Margen Producto")
    total += 1
    
    producto_data = {
        "nombre": "Test Product",
        "precioCompra": 1000,
        "precioVenta": 1500,
        "margenGanancia": 50.0  # ((1500-1000)/1000)*100
    }
    
    result = evaluator(form_type="producto", form_data=producto_data)
    calc_accuracy = result["metrics"]["calculation_accuracy"]
    
    if calc_accuracy >= 0.9:
        print(f"   ‚úÖ C√°lculo margen correcto: {calc_accuracy:.2%}")
        passed += 1
    else:
        print(f"   ‚ùå C√°lculo margen incorrecto: {calc_accuracy:.2%}")
    
    # Test 5: Campos requeridos
    print("\n5Ô∏è‚É£ Test Campos Requeridos")
    total += 1
    
    # Venta sin cliente (faltante requerido)
    venta_incomplete = {"productos": ["PROD001"], "cantidad": 5}
    result = evaluator(form_type="venta", form_data=venta_incomplete)
    field_accuracy = result["metrics"]["field_accuracy"]
    
    if field_accuracy < 1.0:  # Deber√≠a penalizar campo faltante
        print(f"   ‚úÖ Detecci√≥n campos faltantes: {field_accuracy:.2%}")
        passed += 1
    else:
        print(f"   ‚ùå No detect√≥ campos faltantes")
    
    # Resumen
    print("\n" + "-" * 40)
    print(f"   Unit Tests: {passed}/{total} ({(passed/total)*100:.0f}%)")
    
    return passed == total


def main():
    """Punto de entrada principal."""
    print("\n" + "üöÄ" * 35)
    print("   CHRONOS FORM EVALUATION SUITE")
    print("üöÄ" * 35 + "\n")
    
    # 1. Unit tests
    unit_passed = run_unit_tests()
    
    # 2. Integration tests con dataset
    results = run_form_tests()
    
    # 3. Resultado final
    print("\n" + "=" * 70)
    print("üìä RESULTADO FINAL")
    print("=" * 70)
    
    if unit_passed and results.get("pass_rate", 0) >= 80:
        print("\n  üéâ ¬°EVALUACI√ìN COMPLETADA EXITOSAMENTE!")
        print(f"  üìà Tasa de √©xito: {results.get('pass_rate', 0):.1f}%")
        print(f"  ‚úÖ Unit Tests: PASSED")
        return 0
    else:
        print("\n  ‚ö†Ô∏è Evaluaci√≥n completada con advertencias")
        print(f"  üìà Tasa de √©xito: {results.get('pass_rate', 0):.1f}%")
        print(f"  Unit Tests: {'PASSED' if unit_passed else 'FAILED'}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
