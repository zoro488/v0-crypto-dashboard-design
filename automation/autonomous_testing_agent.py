#!/usr/bin/env python3
"""
ðŸ¤– CHRONOS Autonomous Testing Agent
====================================

Agente autÃ³nomo que ejecuta, analiza, corrige y valida tests de forma continua
hasta lograr 100% de cobertura y funcionalidad.

CaracterÃ­sticas:
- EjecuciÃ³n continua de tests
- DetecciÃ³n automÃ¡tica de errores
- GeneraciÃ³n de correcciones
- ValidaciÃ³n de consistencia de datos
- Auto-healing de cÃ³digo
- Reporting en tiempo real

Autor: CHRONOS AI System
VersiÃ³n: 1.0.0
"""

import os
import sys
import json
import time
import subprocess
import re
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum
import traceback

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('automation/logs/agent.log', mode='a')
    ]
)
logger = logging.getLogger(__name__)


class TestStatus(Enum):
    PASSED = "passed"
    FAILED = "failed"
    ERROR = "error"
    SKIPPED = "skipped"
    FIXED = "fixed"


class AgentState(Enum):
    IDLE = "idle"
    TESTING = "testing"
    ANALYZING = "analyzing"
    FIXING = "fixing"
    VALIDATING = "validating"
    COMPLETED = "completed"


@dataclass
class TestResult:
    """Resultado de un test individual."""
    name: str
    status: TestStatus
    duration: float = 0.0
    error_message: Optional[str] = None
    error_trace: Optional[str] = None
    fix_applied: Optional[str] = None
    retries: int = 0


@dataclass
class TestSuite:
    """Conjunto de tests."""
    name: str
    tests: List[TestResult] = field(default_factory=list)
    total: int = 0
    passed: int = 0
    failed: int = 0
    errors: int = 0
    duration: float = 0.0
    
    @property
    def success_rate(self) -> float:
        return (self.passed / self.total * 100) if self.total > 0 else 0.0


@dataclass 
class AgentSession:
    """SesiÃ³n del agente autÃ³nomo."""
    session_id: str
    start_time: datetime
    state: AgentState = AgentState.IDLE
    iterations: int = 0
    max_iterations: int = 50
    test_suites: List[TestSuite] = field(default_factory=list)
    fixes_applied: List[Dict] = field(default_factory=list)
    issues_found: List[Dict] = field(default_factory=list)
    issues_resolved: List[Dict] = field(default_factory=list)
    final_score: float = 0.0
    
    def to_dict(self) -> Dict:
        return {
            "session_id": self.session_id,
            "start_time": self.start_time.isoformat(),
            "state": self.state.value,
            "iterations": self.iterations,
            "fixes_applied": len(self.fixes_applied),
            "issues_found": len(self.issues_found),
            "issues_resolved": len(self.issues_resolved),
            "final_score": self.final_score
        }


class AutonomousTestingAgent:
    """
    Agente autÃ³nomo para testing, correcciÃ³n y validaciÃ³n continua.
    """
    
    def __init__(self, workspace_path: str = "/workspaces/v0-crypto-dashboard-design"):
        self.workspace = Path(workspace_path)
        self.session = AgentSession(
            session_id=f"agent_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            start_time=datetime.now()
        )
        self.target_score = 100.0
        self.min_acceptable_score = 95.0
        
        # Directorios
        self.logs_dir = self.workspace / "automation" / "logs"
        self.reports_dir = self.workspace / "automation" / "reports"
        self.fixes_dir = self.workspace / "automation" / "fixes"
        
        # Crear directorios
        for dir_path in [self.logs_dir, self.reports_dir, self.fixes_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
            
        # Patrones de errores conocidos y sus correcciones
        self.error_patterns = self._load_error_patterns()
        
    def _load_error_patterns(self) -> Dict[str, Dict]:
        """Carga patrones de errores conocidos y sus soluciones."""
        return {
            # TypeScript/JavaScript errors
            r"Cannot find module '([^']+)'": {
                "type": "missing_module",
                "fix": "install_dependency",
                "severity": "high"
            },
            r"Property '(\w+)' does not exist on type": {
                "type": "type_error",
                "fix": "add_type_definition",
                "severity": "medium"
            },
            r"Type '([^']+)' is not assignable to type '([^']+)'": {
                "type": "type_mismatch",
                "fix": "fix_type_annotation",
                "severity": "medium"
            },
            r"'(\w+)' is declared but its value is never read": {
                "type": "unused_variable",
                "fix": "remove_unused",
                "severity": "low"
            },
            # React errors
            r"React Hook useEffect has a missing dependency: '([^']+)'": {
                "type": "missing_dependency",
                "fix": "add_hook_dependency",
                "severity": "medium"
            },
            r"Each child in a list should have a unique \"key\" prop": {
                "type": "missing_key",
                "fix": "add_key_prop",
                "severity": "medium"
            },
            # Firebase errors
            r"FirebaseError: Missing or insufficient permissions": {
                "type": "firebase_permissions",
                "fix": "check_firestore_rules",
                "severity": "high"
            },
            # Test errors
            r"Expected (.*) to (equal|be|match) (.*)": {
                "type": "assertion_failed",
                "fix": "fix_test_expectation",
                "severity": "medium"
            },
            r"Timeout of \d+ms exceeded": {
                "type": "timeout",
                "fix": "increase_timeout_or_optimize",
                "severity": "medium"
            },
            # Python evaluation errors
            r"ModuleNotFoundError: No module named '([^']+)'": {
                "type": "python_missing_module",
                "fix": "pip_install",
                "severity": "high"
            },
            r"AttributeError: '(\w+)' object has no attribute '(\w+)'": {
                "type": "attribute_error",
                "fix": "add_missing_attribute",
                "severity": "high"
            }
        }
    
    def run(self) -> Dict:
        """
        Ejecuta el ciclo completo de testing autÃ³nomo.
        """
        logger.info("=" * 70)
        logger.info("ðŸ¤– CHRONOS Autonomous Testing Agent - Iniciando")
        logger.info("=" * 70)
        
        try:
            while self.session.iterations < self.session.max_iterations:
                self.session.iterations += 1
                logger.info(f"\n{'='*70}")
                logger.info(f"ðŸ“ ITERACIÃ“N {self.session.iterations}/{self.session.max_iterations}")
                logger.info(f"{'='*70}")
                
                # 1. Ejecutar tests
                self.session.state = AgentState.TESTING
                test_results = self._run_all_tests()
                
                # 2. Analizar resultados
                self.session.state = AgentState.ANALYZING
                analysis = self._analyze_results(test_results)
                
                # 3. Verificar si alcanzamos el objetivo
                current_score = analysis.get("overall_score", 0)
                self.session.final_score = current_score
                
                logger.info(f"\nðŸ“Š Score actual: {current_score:.1f}%")
                
                if current_score >= self.target_score:
                    logger.info(f"âœ… Â¡Objetivo alcanzado! Score: {current_score:.1f}%")
                    self.session.state = AgentState.COMPLETED
                    break
                    
                if current_score >= self.min_acceptable_score and self.session.iterations >= 5:
                    logger.info(f"âœ… Score aceptable alcanzado: {current_score:.1f}%")
                    self.session.state = AgentState.COMPLETED
                    break
                
                # 4. Aplicar correcciones
                if analysis.get("issues"):
                    self.session.state = AgentState.FIXING
                    fixes = self._apply_fixes(analysis["issues"])
                    self.session.fixes_applied.extend(fixes)
                    
                # 5. Validar correcciones
                self.session.state = AgentState.VALIDATING
                validation = self._validate_fixes()
                
                # PequeÃ±a pausa entre iteraciones
                time.sleep(1)
                
            # Generar reporte final
            final_report = self._generate_final_report()
            self._save_report(final_report)
            
            return final_report
            
        except Exception as e:
            logger.error(f"Error crÃ­tico en el agente: {e}")
            logger.error(traceback.format_exc())
            return {"error": str(e), "session": self.session.to_dict()}
    
    def _run_all_tests(self) -> Dict[str, TestSuite]:
        """Ejecuta todos los suites de tests."""
        results = {}
        
        # 1. Tests de TypeScript/Lint
        logger.info("\nðŸ” Ejecutando verificaciÃ³n de tipos TypeScript...")
        results["typescript"] = self._run_typescript_check()
        
        # 2. Tests de Python Evaluation
        logger.info("\nðŸ Ejecutando tests de evaluaciÃ³n Python...")
        results["evaluation"] = self._run_python_evaluation()
        
        # 3. Tests de Jest (si existen)
        logger.info("\nðŸ§ª Ejecutando tests de Jest...")
        results["jest"] = self._run_jest_tests()
        
        # 4. ValidaciÃ³n de datos
        logger.info("\nðŸ“Š Validando consistencia de datos...")
        results["data_validation"] = self._run_data_validation()
        
        # 5. ValidaciÃ³n de lÃ³gica de negocio
        logger.info("\nðŸ’¼ Validando lÃ³gica de negocio...")
        results["business_logic"] = self._run_business_logic_validation()
        
        return results
    
    def _run_typescript_check(self) -> TestSuite:
        """Ejecuta verificaciÃ³n de tipos TypeScript."""
        suite = TestSuite(name="TypeScript Type Check")
        
        try:
            result = subprocess.run(
                ["pnpm", "type-check"],
                cwd=str(self.workspace),
                capture_output=True,
                text=True,
                timeout=120
            )
            
            if result.returncode == 0:
                suite.tests.append(TestResult(
                    name="type-check",
                    status=TestStatus.PASSED,
                    duration=0.0
                ))
                suite.passed = 1
            else:
                # Parsear errores
                errors = self._parse_typescript_errors(result.stdout + result.stderr)
                for error in errors:
                    suite.tests.append(TestResult(
                        name=f"type-error-{error['file']}",
                        status=TestStatus.FAILED,
                        error_message=error['message'],
                        error_trace=error.get('trace')
                    ))
                    self.session.issues_found.append({
                        "type": "typescript",
                        "file": error['file'],
                        "message": error['message'],
                        "line": error.get('line')
                    })
                suite.failed = len(errors)
                
            suite.total = max(1, suite.passed + suite.failed)
            
        except subprocess.TimeoutExpired:
            suite.tests.append(TestResult(
                name="type-check",
                status=TestStatus.ERROR,
                error_message="Timeout ejecutando type-check"
            ))
            suite.errors = 1
            suite.total = 1
        except Exception as e:
            suite.tests.append(TestResult(
                name="type-check",
                status=TestStatus.ERROR,
                error_message=str(e)
            ))
            suite.errors = 1
            suite.total = 1
            
        return suite
    
    def _run_python_evaluation(self) -> TestSuite:
        """Ejecuta tests de evaluaciÃ³n Python."""
        suite = TestSuite(name="Python Evaluation Tests")
        
        eval_dir = self.workspace / "evaluation"
        if not eval_dir.exists():
            suite.tests.append(TestResult(
                name="evaluation-setup",
                status=TestStatus.SKIPPED,
                error_message="Directorio de evaluaciÃ³n no encontrado"
            ))
            suite.total = 1
            return suite
            
        try:
            # Ejecutar test_evaluators.py
            result = subprocess.run(
                [sys.executable, "test_evaluators.py"],
                cwd=str(eval_dir),
                capture_output=True,
                text=True,
                timeout=180
            )
            
            # Parsear resultados
            output = result.stdout + result.stderr
            
            # Buscar tests pasados/fallidos
            passed_match = re.search(r'(\d+)/(\d+) tests passed', output)
            if passed_match:
                passed = int(passed_match.group(1))
                total = int(passed_match.group(2))
                suite.passed = passed
                suite.failed = total - passed
                suite.total = total
                
                if passed == total:
                    suite.tests.append(TestResult(
                        name="all-evaluators",
                        status=TestStatus.PASSED
                    ))
                else:
                    # Identificar tests fallidos
                    failed_tests = re.findall(r'âŒ (\w+).*?FAILED', output)
                    for test_name in failed_tests:
                        suite.tests.append(TestResult(
                            name=test_name,
                            status=TestStatus.FAILED,
                            error_message=f"Test {test_name} failed"
                        ))
            else:
                # Si no podemos parsear, verificar el cÃ³digo de retorno
                if result.returncode == 0:
                    suite.passed = 1
                    suite.total = 1
                    suite.tests.append(TestResult(
                        name="evaluation-suite",
                        status=TestStatus.PASSED
                    ))
                else:
                    suite.failed = 1
                    suite.total = 1
                    suite.tests.append(TestResult(
                        name="evaluation-suite",
                        status=TestStatus.FAILED,
                        error_message=output[:500]
                    ))
                    
        except subprocess.TimeoutExpired:
            suite.tests.append(TestResult(
                name="evaluation-suite",
                status=TestStatus.ERROR,
                error_message="Timeout ejecutando evaluaciÃ³n"
            ))
            suite.errors = 1
            suite.total = 1
        except Exception as e:
            suite.tests.append(TestResult(
                name="evaluation-suite",
                status=TestStatus.ERROR,
                error_message=str(e)
            ))
            suite.errors = 1
            suite.total = 1
            
        return suite
    
    def _run_jest_tests(self) -> TestSuite:
        """Ejecuta tests de Jest."""
        suite = TestSuite(name="Jest Unit Tests")
        
        try:
            result = subprocess.run(
                ["pnpm", "test", "--passWithNoTests", "--json"],
                cwd=str(self.workspace),
                capture_output=True,
                text=True,
                timeout=300
            )
            
            # Intentar parsear JSON de Jest
            try:
                # Buscar JSON en la salida
                json_match = re.search(r'\{.*"numTotalTests".*\}', result.stdout, re.DOTALL)
                if json_match:
                    jest_result = json.loads(json_match.group())
                    suite.total = jest_result.get("numTotalTests", 0)
                    suite.passed = jest_result.get("numPassedTests", 0)
                    suite.failed = jest_result.get("numFailedTests", 0)
                else:
                    # Parsear salida estÃ¡ndar
                    if "Tests:" in result.stdout:
                        suite.passed = 1
                        suite.total = 1
            except json.JSONDecodeError:
                if result.returncode == 0:
                    suite.passed = 1
                    suite.total = 1
                    
            if suite.total == 0:
                suite.tests.append(TestResult(
                    name="jest-suite",
                    status=TestStatus.SKIPPED,
                    error_message="No tests encontrados"
                ))
                suite.total = 1
            elif suite.failed == 0:
                suite.tests.append(TestResult(
                    name="jest-suite",
                    status=TestStatus.PASSED
                ))
            else:
                suite.tests.append(TestResult(
                    name="jest-suite",
                    status=TestStatus.FAILED,
                    error_message=f"{suite.failed} tests fallidos"
                ))
                
        except subprocess.TimeoutExpired:
            suite.tests.append(TestResult(
                name="jest-suite",
                status=TestStatus.ERROR,
                error_message="Timeout ejecutando Jest"
            ))
            suite.errors = 1
            suite.total = 1
        except Exception as e:
            suite.tests.append(TestResult(
                name="jest-suite",
                status=TestStatus.SKIPPED,
                error_message=f"Jest no disponible: {e}"
            ))
            suite.total = 1
            
        return suite
    
    def _run_data_validation(self) -> TestSuite:
        """Valida consistencia de datos en el sistema."""
        suite = TestSuite(name="Data Validation")
        
        # Validar que existen los archivos CSV de datos
        csv_dir = self.workspace / "csv"
        if csv_dir.exists():
            csv_files = list(csv_dir.glob("*.csv"))
            suite.total = len(csv_files)
            
            for csv_file in csv_files:
                try:
                    with open(csv_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        if len(lines) > 0:
                            suite.tests.append(TestResult(
                                name=f"csv-{csv_file.stem}",
                                status=TestStatus.PASSED
                            ))
                            suite.passed += 1
                        else:
                            suite.tests.append(TestResult(
                                name=f"csv-{csv_file.stem}",
                                status=TestStatus.FAILED,
                                error_message="Archivo vacÃ­o"
                            ))
                            suite.failed += 1
                except Exception as e:
                    suite.tests.append(TestResult(
                        name=f"csv-{csv_file.stem}",
                        status=TestStatus.ERROR,
                        error_message=str(e)
                    ))
                    suite.errors += 1
        else:
            suite.tests.append(TestResult(
                name="csv-directory",
                status=TestStatus.SKIPPED,
                error_message="Directorio CSV no encontrado"
            ))
            suite.total = 1
            
        return suite
    
    def _run_business_logic_validation(self) -> TestSuite:
        """Valida la lÃ³gica de negocio del sistema."""
        suite = TestSuite(name="Business Logic Validation")
        
        eval_dir = self.workspace / "evaluation"
        business_logic_test = eval_dir / "evaluators" / "business_logic.py"
        
        if business_logic_test.exists():
            try:
                result = subprocess.run(
                    [sys.executable, "-c", f"""
import sys
sys.path.insert(0, '{eval_dir}')
from evaluators.business_logic import BusinessLogicEvaluator

evaluator = BusinessLogicEvaluator()

# Test distribuciÃ³n de ventas
test_cases = [
    {{"precioVenta": 10000, "precioCompra": 6300, "precioFlete": 500, "cantidad": 10}},
    {{"precioVenta": 15000, "precioCompra": 9000, "precioFlete": 600, "cantidad": 5}},
    {{"precioVenta": 8000, "precioCompra": 5000, "precioFlete": 400, "cantidad": 20}},
]

passed = 0
for tc in test_cases:
    result = evaluator(
        operation_type="venta",
        input_data=tc,
        output_data={{
            "distribucion": {{
                "boveda_monte": tc["precioCompra"] * tc["cantidad"],
                "fletes": tc["precioFlete"] * tc["cantidad"],
                "utilidades": (tc["precioVenta"] - tc["precioCompra"] - tc["precioFlete"]) * tc["cantidad"]
            }},
            "total": tc["precioVenta"] * tc["cantidad"]
        }}
    )
    if result.get("distribution_accuracy", 0) == 1.0:
        passed += 1

print(f"Business Logic: {{passed}}/{{len(test_cases)}} tests passed")
if passed == len(test_cases):
    print("ALL_PASSED")
"""],
                    cwd=str(self.workspace),
                    capture_output=True,
                    text=True,
                    timeout=60
                )
                
                if "ALL_PASSED" in result.stdout:
                    suite.tests.append(TestResult(
                        name="sales-distribution",
                        status=TestStatus.PASSED
                    ))
                    suite.passed = 1
                else:
                    suite.tests.append(TestResult(
                        name="sales-distribution",
                        status=TestStatus.FAILED,
                        error_message=result.stdout + result.stderr
                    ))
                    suite.failed = 1
                    
                suite.total = 1
                
            except Exception as e:
                suite.tests.append(TestResult(
                    name="business-logic",
                    status=TestStatus.ERROR,
                    error_message=str(e)
                ))
                suite.errors = 1
                suite.total = 1
        else:
            suite.tests.append(TestResult(
                name="business-logic",
                status=TestStatus.SKIPPED,
                error_message="Evaluador de lÃ³gica de negocio no encontrado"
            ))
            suite.total = 1
            
        return suite
    
    def _parse_typescript_errors(self, output: str) -> List[Dict]:
        """Parsea errores de TypeScript."""
        errors = []
        
        # PatrÃ³n de error TypeScript
        pattern = r"([^:\s]+\.tsx?):(\d+):(\d+)\s*-\s*error\s*(TS\d+):\s*(.+)"
        matches = re.findall(pattern, output)
        
        for match in matches:
            errors.append({
                "file": match[0],
                "line": int(match[1]),
                "column": int(match[2]),
                "code": match[3],
                "message": match[4]
            })
            
        return errors[:20]  # Limitar a 20 errores
    
    def _analyze_results(self, results: Dict[str, TestSuite]) -> Dict:
        """Analiza los resultados de todos los tests."""
        analysis = {
            "overall_score": 0.0,
            "suites": {},
            "issues": [],
            "recommendations": []
        }
        
        total_tests = 0
        total_passed = 0
        
        for suite_name, suite in results.items():
            total_tests += suite.total
            total_passed += suite.passed
            
            analysis["suites"][suite_name] = {
                "total": suite.total,
                "passed": suite.passed,
                "failed": suite.failed,
                "errors": suite.errors,
                "success_rate": suite.success_rate
            }
            
            # Recopilar issues
            for test in suite.tests:
                if test.status in [TestStatus.FAILED, TestStatus.ERROR]:
                    analysis["issues"].append({
                        "suite": suite_name,
                        "test": test.name,
                        "status": test.status.value,
                        "message": test.error_message,
                        "trace": test.error_trace
                    })
        
        # Calcular score global
        if total_tests > 0:
            analysis["overall_score"] = (total_passed / total_tests) * 100
            
        # Agregar recomendaciones basadas en issues
        if analysis["issues"]:
            for issue in analysis["issues"]:
                rec = self._generate_recommendation(issue)
                if rec:
                    analysis["recommendations"].append(rec)
                    
        return analysis
    
    def _generate_recommendation(self, issue: Dict) -> Optional[Dict]:
        """Genera una recomendaciÃ³n para resolver un issue."""
        message = issue.get("message", "")
        
        for pattern, fix_info in self.error_patterns.items():
            match = re.search(pattern, message)
            if match:
                return {
                    "issue": issue["test"],
                    "type": fix_info["type"],
                    "fix_type": fix_info["fix"],
                    "severity": fix_info["severity"],
                    "match_groups": match.groups() if match.groups() else None
                }
                
        return None
    
    def _apply_fixes(self, issues: List[Dict]) -> List[Dict]:
        """Aplica correcciones automÃ¡ticas a los issues encontrados."""
        fixes_applied = []
        
        for issue in issues:
            fix_result = self._try_auto_fix(issue)
            if fix_result:
                fixes_applied.append(fix_result)
                self.session.issues_resolved.append(issue)
                logger.info(f"  âœ… CorrecciÃ³n aplicada: {fix_result['description']}")
            else:
                logger.warning(f"  âš ï¸ No se pudo corregir automÃ¡ticamente: {issue.get('test', 'unknown')}")
                
        return fixes_applied
    
    def _try_auto_fix(self, issue: Dict) -> Optional[Dict]:
        """Intenta aplicar una correcciÃ³n automÃ¡tica."""
        # Por ahora, registrar el issue para correcciÃ³n manual
        # En una implementaciÃ³n completa, aquÃ­ irÃ­a la lÃ³gica de auto-fix
        
        message = issue.get("message", "")
        
        # CorrecciÃ³n para mÃ³dulos Python faltantes
        if "ModuleNotFoundError" in message:
            module_match = re.search(r"No module named '([^']+)'", message)
            if module_match:
                module = module_match.group(1)
                try:
                    subprocess.run(
                        [sys.executable, "-m", "pip", "install", module],
                        capture_output=True,
                        timeout=60
                    )
                    return {
                        "type": "pip_install",
                        "description": f"Instalado mÃ³dulo Python: {module}",
                        "module": module
                    }
                except Exception:
                    pass
                    
        return None
    
    def _validate_fixes(self) -> Dict:
        """Valida que las correcciones aplicadas funcionan."""
        return {
            "validated": True,
            "fixes_verified": len(self.session.fixes_applied)
        }
    
    def _generate_final_report(self) -> Dict:
        """Genera el reporte final de la sesiÃ³n."""
        duration = (datetime.now() - self.session.start_time).total_seconds()
        
        report = {
            "session": self.session.to_dict(),
            "duration_seconds": duration,
            "duration_formatted": f"{int(duration // 60)}m {int(duration % 60)}s",
            "final_score": self.session.final_score,
            "target_score": self.target_score,
            "target_achieved": self.session.final_score >= self.target_score,
            "summary": {
                "total_iterations": self.session.iterations,
                "issues_found": len(self.session.issues_found),
                "issues_resolved": len(self.session.issues_resolved),
                "fixes_applied": len(self.session.fixes_applied)
            },
            "timestamp": datetime.now().isoformat()
        }
        
        # Log resumen
        logger.info("\n" + "=" * 70)
        logger.info("ðŸ“‹ REPORTE FINAL")
        logger.info("=" * 70)
        logger.info(f"  Score Final: {self.session.final_score:.1f}%")
        logger.info(f"  Objetivo: {self.target_score}%")
        logger.info(f"  Alcanzado: {'âœ… SÃ' if report['target_achieved'] else 'âŒ NO'}")
        logger.info(f"  Iteraciones: {self.session.iterations}")
        logger.info(f"  Issues encontrados: {len(self.session.issues_found)}")
        logger.info(f"  Issues resueltos: {len(self.session.issues_resolved)}")
        logger.info(f"  DuraciÃ³n: {report['duration_formatted']}")
        logger.info("=" * 70)
        
        return report
    
    def _save_report(self, report: Dict):
        """Guarda el reporte en archivo."""
        report_file = self.reports_dir / f"report_{self.session.session_id}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        logger.info(f"ðŸ“ Reporte guardado: {report_file}")


def main():
    """Punto de entrada principal."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ðŸ¤– CHRONOS Autonomous Testing Agent v1.0.0                  â•‘
â•‘                                                                      â•‘
â•‘  Agente autÃ³nomo para testing, correcciÃ³n y validaciÃ³n continua     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    agent = AutonomousTestingAgent()
    result = agent.run()
    
    # CÃ³digo de salida basado en resultado
    if result.get("target_achieved", False):
        sys.exit(0)
    elif result.get("final_score", 0) >= 95:
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()
