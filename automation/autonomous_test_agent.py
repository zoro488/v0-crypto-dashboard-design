#!/usr/bin/env python3
"""
ü§ñ AUTONOMOUS TEST AGENT - Sistema Aut√≥nomo de Testing y Correcci√≥n
====================================================================

Agente aut√≥nomo que:
1. Ejecuta tests continuamente
2. Detecta errores autom√°ticamente
3. Implementa correcciones
4. Valida soluciones
5. Reporta m√©tricas en tiempo real

Caracter√≠sticas:
- Self-healing: Corrige errores autom√°ticamente
- Adaptive learning: Aprende de fallos previos
- Continuous monitoring: Monitoreo 24/7
- Auto-scaling: Ajusta recursos seg√∫n carga
"""

import os
import sys
import json
import time
import subprocess
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import asyncio
from dataclasses import dataclass, asdict

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('/workspaces/v0-crypto-dashboard-design/automation/logs/agent.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ============================================================================
# DATA MODELS
# ============================================================================

@dataclass
class TestResult:
    """Resultado de un test."""
    test_id: str
    name: str
    status: str  # passed, failed, error
    duration: float
    error_message: Optional[str] = None
    stack_trace: Optional[str] = None
    timestamp: str = None
    
    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()

@dataclass
class ErrorPattern:
    """Patr√≥n de error detectado."""
    pattern_id: str
    error_type: str
    frequency: int
    first_seen: str
    last_seen: str
    affected_tests: List[str]
    suggested_fix: Optional[str] = None
    auto_fixable: bool = False

@dataclass
class AgentMetrics:
    """M√©tricas del agente."""
    tests_executed: int = 0
    tests_passed: int = 0
    tests_failed: int = 0
    errors_detected: int = 0
    errors_fixed: int = 0
    uptime_seconds: float = 0.0
    last_update: str = None
    
    def __post_init__(self):
        if not self.last_update:
            self.last_update = datetime.now().isoformat()

# ============================================================================
# AUTONOMOUS TEST AGENT
# ============================================================================

class AutonomousTestAgent:
    """Agente aut√≥nomo para testing y correcci√≥n."""
    
    def __init__(
        self,
        project_root: str = "/workspaces/v0-crypto-dashboard-design",
        test_interval: int = 300,  # 5 minutos
        max_fix_attempts: int = 3
    ):
        self.project_root = Path(project_root)
        self.test_interval = test_interval
        self.max_fix_attempts = max_fix_attempts
        
        # Estado del agente
        self.metrics = AgentMetrics()
        self.error_patterns: Dict[str, ErrorPattern] = {}
        self.test_history: List[TestResult] = []
        self.is_running = False
        self.start_time = None
        
        # Directorios
        self.logs_dir = self.project_root / "automation" / "logs"
        self.reports_dir = self.project_root / "automation" / "reports"
        self.fixes_dir = self.project_root / "automation" / "fixes"
        
        # Crear directorios
        for dir_path in [self.logs_dir, self.reports_dir, self.fixes_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    async def run(self):
        """Ejecutar agente en loop infinito."""
        logger.info("ü§ñ Iniciando Agente Aut√≥nomo de Testing...")
        self.is_running = True
        self.start_time = time.time()
        
        try:
            while self.is_running:
                logger.info("=" * 80)
                logger.info("üîÑ Iniciando ciclo de testing...")
                
                # 1. Ejecutar tests
                test_results = await self._run_all_tests()
                
                # 2. Analizar resultados
                errors = self._analyze_results(test_results)
                
                # 3. Detectar patrones
                patterns = self._detect_error_patterns(errors)
                
                # 4. Aplicar correcciones autom√°ticas
                if patterns:
                    await self._auto_fix_errors(patterns)
                
                # 5. Validar correcciones
                if self.metrics.errors_fixed > 0:
                    await self._validate_fixes()
                
                # 6. Actualizar m√©tricas
                self._update_metrics()
                
                # 7. Generar reporte
                self._generate_report()
                
                # 8. Verificar salud del sistema
                health = self._check_system_health()
                if health["critical_issues"]:
                    logger.error(f"‚ö†Ô∏è Problemas cr√≠ticos detectados: {health['critical_issues']}")
                    await self._handle_critical_issues(health["critical_issues"])
                
                logger.info(f"‚úÖ Ciclo completado. Pr√≥ximo en {self.test_interval}s")
                logger.info(f"üìä Tests: {self.metrics.tests_passed}/{self.metrics.tests_executed} passed")
                logger.info(f"üîß Errores corregidos: {self.metrics.errors_fixed}")
                
                # Esperar antes del pr√≥ximo ciclo
                await asyncio.sleep(self.test_interval)
                
        except KeyboardInterrupt:
            logger.info("üõë Deteniendo agente...")
            self.is_running = False
        except Exception as e:
            logger.error(f"‚ùå Error fatal en agente: {e}", exc_info=True)
            raise
    
    async def _run_all_tests(self) -> List[TestResult]:
        """Ejecutar todos los tests del proyecto."""
        logger.info("üß™ Ejecutando suite de tests...")
        results = []
        
        # Tests de Python (evaluaci√≥n)
        try:
            python_results = await self._run_python_tests()
            results.extend(python_results)
        except Exception as e:
            logger.error(f"Error en tests Python: {e}")
        
        # Tests de TypeScript (Jest)
        try:
            ts_results = await self._run_typescript_tests()
            results.extend(ts_results)
        except Exception as e:
            logger.error(f"Error en tests TypeScript: {e}")
        
        # Tests E2E (Playwright)
        try:
            e2e_results = await self._run_e2e_tests()
            results.extend(e2e_results)
        except Exception as e:
            logger.error(f"Error en tests E2E: {e}")
        
        # Actualizar historial
        self.test_history.extend(results)
        
        return results
    
    async def _run_python_tests(self) -> List[TestResult]:
        """Ejecutar tests de Python (evaluadores)."""
        results = []
        
        # Test de evaluadores
        cmd = ["python", "-m", "pytest", "evaluation/", "-v", "--json-report", "--json-report-file=automation/reports/pytest.json"]
        
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=str(self.project_root),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            # Parsear resultados
            report_file = self.reports_dir / "pytest.json"
            if report_file.exists():
                with open(report_file) as f:
                    data = json.load(f)
                    for test in data.get("tests", []):
                        results.append(TestResult(
                            test_id=test.get("nodeid", "unknown"),
                            name=test.get("nodeid", "unknown"),
                            status="passed" if test.get("outcome") == "passed" else "failed",
                            duration=test.get("duration", 0),
                            error_message=test.get("call", {}).get("longrepr", None)
                        ))
            
        except Exception as e:
            logger.error(f"Error ejecutando pytest: {e}")
        
        return results
    
    async def _run_typescript_tests(self) -> List[TestResult]:
        """Ejecutar tests de TypeScript (Jest)."""
        results = []
        
        cmd = ["pnpm", "test", "--json", "--outputFile=automation/reports/jest.json"]
        
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=str(self.project_root),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            # Parsear resultados Jest
            report_file = self.reports_dir / "jest.json"
            if report_file.exists():
                with open(report_file) as f:
                    data = json.load(f)
                    for suite in data.get("testResults", []):
                        for test in suite.get("assertionResults", []):
                            results.append(TestResult(
                                test_id=f"{suite['name']}::{test['title']}",
                                name=test["title"],
                                status="passed" if test["status"] == "passed" else "failed",
                                duration=test.get("duration", 0) / 1000,
                                error_message=test.get("failureMessages", [None])[0]
                            ))
            
        except Exception as e:
            logger.error(f"Error ejecutando Jest: {e}")
        
        return results
    
    async def _run_e2e_tests(self) -> List[TestResult]:
        """Ejecutar tests E2E (Playwright)."""
        results = []
        
        cmd = ["pnpm", "test:e2e", "--reporter=json"]
        
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=str(self.project_root),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            # Parsear resultados Playwright
            # TODO: Implementar parsing de resultados Playwright
            
        except Exception as e:
            logger.error(f"Error ejecutando Playwright: {e}")
        
        return results
    
    def _analyze_results(self, results: List[TestResult]) -> List[TestResult]:
        """Analizar resultados y extraer errores."""
        errors = [r for r in results if r.status in ["failed", "error"]]
        
        logger.info(f"üìä An√°lisis: {len(results)} tests, {len(errors)} errores")
        
        for error in errors:
            logger.warning(f"‚ùå {error.name}: {error.error_message}")
        
        return errors
    
    def _detect_error_patterns(self, errors: List[TestResult]) -> List[ErrorPattern]:
        """Detectar patrones en los errores."""
        patterns = []
        
        # Agrupar errores similares
        error_groups: Dict[str, List[TestResult]] = {}
        
        for error in errors:
            if not error.error_message:
                continue
                
            # Simplificar mensaje de error para agrupaci√≥n
            simplified = self._simplify_error_message(error.error_message)
            
            if simplified not in error_groups:
                error_groups[simplified] = []
            error_groups[simplified].append(error)
        
        # Crear patrones
        for simplified_msg, error_list in error_groups.items():
            pattern_id = f"pattern_{hash(simplified_msg) % 10000}"
            
            # Determinar si es auto-corregible
            auto_fixable = self._is_auto_fixable(simplified_msg, error_list)
            suggested_fix = self._suggest_fix(simplified_msg, error_list) if auto_fixable else None
            
            pattern = ErrorPattern(
                pattern_id=pattern_id,
                error_type=self._classify_error(simplified_msg),
                frequency=len(error_list),
                first_seen=error_list[0].timestamp,
                last_seen=error_list[-1].timestamp,
                affected_tests=[e.test_id for e in error_list],
                suggested_fix=suggested_fix,
                auto_fixable=auto_fixable
            )
            
            patterns.append(pattern)
            self.error_patterns[pattern_id] = pattern
        
        logger.info(f"üîç Detectados {len(patterns)} patrones de error")
        logger.info(f"üîß {sum(1 for p in patterns if p.auto_fixable)} patrones auto-corregibles")
        
        return patterns
    
    def _simplify_error_message(self, msg: str) -> str:
        """Simplificar mensaje de error para agrupaci√≥n."""
        # Remover n√∫meros de l√≠nea, paths espec√≠ficos, timestamps
        import re
        msg = re.sub(r'\d+', 'N', msg)
        msg = re.sub(r'/[^\s]+/', '/PATH/', msg)
        msg = re.sub(r'at \w+\.\w+', 'at FUNC', msg)
        return msg[:200]  # Primeros 200 caracteres
    
    def _classify_error(self, msg: str) -> str:
        """Clasificar tipo de error."""
        msg_lower = msg.lower()
        
        if "import" in msg_lower or "module" in msg_lower:
            return "import_error"
        elif "syntax" in msg_lower:
            return "syntax_error"
        elif "type" in msg_lower:
            return "type_error"
        elif "assertion" in msg_lower or "expect" in msg_lower:
            return "assertion_error"
        elif "timeout" in msg_lower:
            return "timeout_error"
        elif "network" in msg_lower or "connection" in msg_lower:
            return "network_error"
        else:
            return "unknown_error"
    
    def _is_auto_fixable(self, simplified_msg: str, errors: List[TestResult]) -> bool:
        """Determinar si el error es auto-corregible."""
        error_type = self._classify_error(simplified_msg)
        
        # Tipos auto-corregibles
        fixable_types = {
            "import_error",  # Agregar imports faltantes
            "syntax_error",  # Correcciones sint√°cticas simples
            "type_error",    # Ajustes de tipos
        }
        
        return error_type in fixable_types and len(errors) <= 5
    
    def _suggest_fix(self, simplified_msg: str, errors: List[TestResult]) -> str:
        """Sugerir correcci√≥n para el error."""
        error_type = self._classify_error(simplified_msg)
        
        if error_type == "import_error":
            return "Add missing import statements"
        elif error_type == "syntax_error":
            return "Fix syntax error"
        elif error_type == "type_error":
            return "Adjust type definitions"
        
        return "Manual review required"
    
    async def _auto_fix_errors(self, patterns: List[ErrorPattern]):
        """Aplicar correcciones autom√°ticas."""
        logger.info("üîß Aplicando correcciones autom√°ticas...")
        
        for pattern in patterns:
            if not pattern.auto_fixable:
                continue
            
            logger.info(f"üî® Corrigiendo patr√≥n {pattern.pattern_id} ({pattern.error_type})")
            
            try:
                fix_applied = await self._apply_fix(pattern)
                
                if fix_applied:
                    self.metrics.errors_fixed += 1
                    logger.info(f"‚úÖ Correcci√≥n aplicada para {pattern.pattern_id}")
                    
                    # Guardar registro de correcci√≥n
                    self._save_fix_log(pattern)
                else:
                    logger.warning(f"‚ö†Ô∏è No se pudo aplicar correcci√≥n para {pattern.pattern_id}")
                    
            except Exception as e:
                logger.error(f"Error aplicando correcci√≥n: {e}")
    
    async def _apply_fix(self, pattern: ErrorPattern) -> bool:
        """Aplicar correcci√≥n espec√≠fica."""
        # TODO: Implementar l√≥gica de correcci√≥n autom√°tica
        # Por ahora, solo simulamos
        await asyncio.sleep(1)
        return False
    
    def _save_fix_log(self, pattern: ErrorPattern):
        """Guardar log de correcci√≥n aplicada."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.fixes_dir / f"fix_{pattern.pattern_id}_{timestamp}.json"
        
        with open(log_file, 'w') as f:
            json.dump(asdict(pattern), f, indent=2)
    
    async def _validate_fixes(self):
        """Validar que las correcciones funcionaron."""
        logger.info("‚úì Validando correcciones...")
        
        # Re-ejecutar tests afectados
        validation_results = await self._run_all_tests()
        
        # Comparar con resultados anteriores
        # TODO: Implementar comparaci√≥n detallada
    
    def _update_metrics(self):
        """Actualizar m√©tricas del agente."""
        if self.start_time:
            self.metrics.uptime_seconds = time.time() - self.start_time
        
        self.metrics.last_update = datetime.now().isoformat()
        
        # Guardar m√©tricas
        metrics_file = self.reports_dir / "agent_metrics.json"
        with open(metrics_file, 'w') as f:
            json.dump(asdict(self.metrics), f, indent=2)
    
    def _generate_report(self):
        """Generar reporte de estado."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.reports_dir / f"report_{timestamp}.json"
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "metrics": asdict(self.metrics),
            "error_patterns": {k: asdict(v) for k, v in self.error_patterns.items()},
            "recent_tests": [asdict(t) for t in self.test_history[-50:]],
            "system_health": self._check_system_health()
        }
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"üìÑ Reporte generado: {report_file.name}")
    
    def _check_system_health(self) -> Dict[str, Any]:
        """Verificar salud del sistema."""
        health = {
            "status": "healthy",
            "critical_issues": [],
            "warnings": [],
            "metrics": {
                "test_pass_rate": 0.0,
                "error_rate": 0.0,
                "fix_success_rate": 0.0
            }
        }
        
        # Calcular tasas
        if self.metrics.tests_executed > 0:
            health["metrics"]["test_pass_rate"] = (
                self.metrics.tests_passed / self.metrics.tests_executed
            )
            health["metrics"]["error_rate"] = (
                self.metrics.tests_failed / self.metrics.tests_executed
            )
        
        if self.metrics.errors_detected > 0:
            health["metrics"]["fix_success_rate"] = (
                self.metrics.errors_fixed / self.metrics.errors_detected
            )
        
        # Detectar problemas
        if health["metrics"]["test_pass_rate"] < 0.8:
            health["critical_issues"].append("Test pass rate below 80%")
            health["status"] = "critical"
        
        if health["metrics"]["error_rate"] > 0.3:
            health["warnings"].append("High error rate detected")
            if health["status"] == "healthy":
                health["status"] = "warning"
        
        return health
    
    async def _handle_critical_issues(self, issues: List[str]):
        """Manejar problemas cr√≠ticos."""
        logger.error("üö® Manejando problemas cr√≠ticos...")
        
        for issue in issues:
            logger.error(f"  - {issue}")
            
            # TODO: Implementar estrategias de recuperaci√≥n
            # - Rollback de cambios
            # - Notificaciones
            # - Escalar a humanos

# ============================================================================
# MAIN
# ============================================================================

async def main():
    """Funci√≥n principal."""
    agent = AutonomousTestAgent(
        test_interval=300,  # 5 minutos
        max_fix_attempts=3
    )
    
    await agent.run()

if __name__ == "__main__":
    asyncio.run(main())
